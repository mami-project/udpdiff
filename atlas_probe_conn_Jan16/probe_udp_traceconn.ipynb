{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDP Traceroute Connectivity by Probe\n",
    "\n",
    "This notebook uses _existing_ RIPE Atlas measurements to determine how many RIPE Atlas probes might have issues with UDP connectivity. The basic methodology is as follows:\n",
    "\n",
    "- Examine measurement metadata to find UDP traceroutes from many probes, over time (currently measurements ending in 2015, or still ongoing).\n",
    "- Take latency to target and target reachability information from each sample. For samples where the target is not reached but some hops did respond, take the latest hop.\n",
    "- Group reachability information by probe, and classify probes by how confident we are UDP is broken on them.\n",
    "\n",
    "This notebook searches the RIPE atlas measurement metadata archive (available from [ftp.ripe.net](ftp://ftp.ripe.net/atlas/measurements)) for UDP and TCP traceroute measurements, and uses the Atlas API to download measurements and cache them locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "Set up the environment and define functions we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some jupyter magic to set up the environment correctly\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# thanks for letting me know about your plans but i don't really care\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "\n",
    "# things we need, things to make us go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import warnings\n",
    "import hashlib\n",
    "import requests\n",
    "import os.path\n",
    "import time\n",
    "import calendar\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "# HDF5 store for stashing parsed things\n",
    "store = pd.HDFStore('data_cache/udp_traceconn.h5')\n",
    "\n",
    "##############################################################################\n",
    "# Trace metadata named tuple, probe to MSM mapping named tuple,\n",
    "# and generator for extracting these from the all-measurements-fjson.txt file.\n",
    "##############################################################################\n",
    "\n",
    "TraceMeta = collections.namedtuple(\"TraceMeta\", \n",
    "                (\"msm_id\", \"af\", \"proto\", \"target\", \n",
    "                 \"start_epoch\", \"stop_epoch\", \"interval\", \"probes\"))\n",
    "\n",
    "MSMProbeMeta = collections.namedtuple(\"MSMProbeMeta\", (\"msm_id\", \"pid\"))\n",
    "\n",
    "def tm_generator(first_start=0, last_start=calendar.timegm(time.gmtime()), skip_lines = 0):\n",
    "    current_time = calendar.timegm(time.gmtime())\n",
    "    \n",
    "    with open(\"data_cache/all-measurements-fjson.txt\") as fjf:\n",
    "        for num, line in enumerate(fjf):\n",
    "            if num < skip_lines:\n",
    "                continue\n",
    "            \n",
    "            mm = json.loads(line)\n",
    "                \n",
    "            if (((mm['type']['id'] == 2) or (mm['type']['id'] == 4)) and\n",
    "                ((mm['status']['id'] == 2) or (mm['status']['id'] == 4)) and\n",
    "                (('protocol' in mm) or ('proto_tcp' in mm)) and\n",
    "                (mm['start_time'] >= first_start) and\n",
    "                (mm['start_time'] < last_start) ):\n",
    "            \n",
    "                    if mm['status']['id'] == 4:\n",
    "                        stop_time = mm['stop_time']\n",
    "                    else:\n",
    "                        stop_time = current_time\n",
    "                    \n",
    "                    if 'proto_tcp' in mm and mm['proto_tcp']:\n",
    "                        proto = 'TCP'\n",
    "                    else:\n",
    "                        proto = mm['protocol']\n",
    "                        \n",
    "                    yield TraceMeta(mm['msm_id'], mm['af'], proto, mm['dst_addr'], \n",
    "                                    mm['start_time'], stop_time, mm['interval'],\n",
    "                                    mm['participant_count'])\n",
    "\n",
    "def mpm_generator(msm_ids, skip_lines = 0):\n",
    "    current_time = calendar.timegm(time.gmtime())\n",
    "\n",
    "    with open(\"data_cache/all-measurements-fjson.txt\") as fjf:\n",
    "        for num, line in enumerate(fjf):\n",
    "            if num < skip_lines:\n",
    "                continue\n",
    "        \n",
    "            mm = json.loads(line)\n",
    "            if (mm['msm_id'] in msm_ids):\n",
    "                if \"probes\" in mm:\n",
    "                    for p in mm[\"probes\"]:\n",
    "                        yield MSMProbeMeta(mm['msm_id'], p['id'])\n",
    "                        \n",
    "def mpm_dataframe(msm_ids, chunksize=1000000):\n",
    "    # initialize MPM generator\n",
    "    mpmgen = mpm_generator(msm_ids)\n",
    "    \n",
    "    # initialize accumulators\n",
    "    df = pd.DataFrame([], columns=MSMProbeMeta._fields)\n",
    "    data = []\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # get individual rows from get_msm\n",
    "    for mpm in mpmgen:\n",
    "        data.append(mpm)\n",
    "        if len(data) >= chunksize:\n",
    "            df = df.append(pd.DataFrame(data, columns=MSMProbeMeta._fields), ignore_index=True)\n",
    "            data = []\n",
    "            print(\"chunk \"+str(chunk_count))\n",
    "            chunk_count += 1\n",
    "                    \n",
    "    # Append final dataframe if non-empty.\n",
    "    if len(data) > 0:\n",
    "        df = df.append(pd.DataFrame(data, columns=MSMProbeMeta._fields), ignore_index=True)\n",
    "\n",
    "    # Close generator (close underlying file)\n",
    "    mpmgen.close()\n",
    "    \n",
    "    # Return dataframe\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# Probe metadata named tuple and extraction code\n",
    "##############################################################################\n",
    "AtlasProbe = collections.namedtuple(\"AtlasProbe\",\n",
    "           (\"pid\", \"version\", \"nat\", \"ip4\", \"ip6\", \"asn4\", \"asn6\", \"cc\", \"lat\", \"lon\"))\n",
    "\n",
    "def extract_atlas_probe(pobj):\n",
    "    if \"address_v4\" in pobj:\n",
    "        ip4 = pobj[\"address_v4\"]\n",
    "    else:\n",
    "        ip4 = None\n",
    "\n",
    "    if \"address_v6\" in pobj:\n",
    "        ip6 = pobj[\"address_v6\"]\n",
    "    else:\n",
    "        ip6 = None\n",
    "\n",
    "    if \"asn_v4\" in pobj:\n",
    "        asn4 = pobj[\"asn_v4\"]\n",
    "    else:\n",
    "        asn4 = None\n",
    "\n",
    "    if \"asn_v6\" in pobj:\n",
    "        asn6 = pobj[\"asn_v6\"]\n",
    "    else:\n",
    "        asn6 = None\n",
    "\n",
    "    if \"tags\" in pobj:\n",
    "        if \"system-v1\" in pobj[\"tags\"]:\n",
    "            version = 1\n",
    "        elif \"system-v2\" in pobj[\"tags\"]:\n",
    "            version = 2\n",
    "        elif \"system-v3\" in pobj[\"tags\"]:\n",
    "            version = 3\n",
    "        elif \"system-anchor\" in pobj[\"tags\"]:\n",
    "            version = 4\n",
    "        else:\n",
    "            version = 0\n",
    "\n",
    "        nat = \"nat\" in pobj[\"tags\"]\n",
    "\n",
    "\n",
    "    return AtlasProbe(pobj[\"id\"], version, nat, ip4, ip6, asn4, asn6,\n",
    "                      pobj[\"country_code\"], pobj[\"latitude\"], pobj[\"longitude\"])       \n",
    "\n",
    "def probe_dataframe_from_file(filename=\"data_cache/all-probes.json\"):\n",
    "    data = []\n",
    "    \n",
    "    # make a giant array\n",
    "    with open(filename) as stream:\n",
    "        all_probes = json.loads(stream.read())\n",
    "        for pobj in all_probes[\"objects\"]:\n",
    "            data.append(extract_atlas_probe(pobj))\n",
    "\n",
    "    # create a dataframe from it\n",
    "    df = pd.DataFrame(data, columns=AtlasProbe._fields)\n",
    "    \n",
    "    # indexed by probe ID\n",
    "    df.index = df['pid']\n",
    "    del(df['pid'])\n",
    "    \n",
    "    # stick an aggregation column on there, we'll use it later\n",
    "    df['n'] = 1\n",
    "    \n",
    "    # and return it\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# MSM retrieval code\n",
    "##############################################################################\n",
    "\n",
    "def get_msm(msm_id, gen, cachedir=None, start=None, stop=None):\n",
    "    \"\"\"\n",
    "    Given an MSM, fetch it from the cache or from the RIPE Atlas API.\n",
    "    Yield each separate result according to the generation function.\n",
    "    \"\"\"\n",
    "    url = \"https://atlas.ripe.net/api/v1/measurement/%u/result/\" % (msm_id,)\n",
    "\n",
    "    params = {\"format\": \"json\"}\n",
    "    if start is not None and stop is not None:\n",
    "        params[\"start\"] = str(start)\n",
    "        params[\"stop\"] = str(stop)\n",
    "    \n",
    "    if cachedir and os.path.isdir(cachedir):\n",
    "        filepath = os.path.join(cachedir, \"measurement\", \"%u.json\" % (msm_id,))\n",
    "\n",
    "        # download if not present\n",
    "        if not os.path.isfile(filepath):\n",
    "            with open(filepath, mode=\"wb\") as file:\n",
    "                print(\"Cache miss, retrieving \"+url)\n",
    "                res = requests.get(url, params=params)\n",
    "\n",
    "                if not res.ok:\n",
    "                    raise \"Atlas measurement API request failed: \"+repr(res.json())\n",
    "                \n",
    "                file.write(res.content)\n",
    "\n",
    "        # then read from cache\n",
    "        with open(filepath) as stream:\n",
    "            yield from gen(json.loads(stream.read()))\n",
    "\n",
    "    else:\n",
    "        # just read from the net\n",
    "        res = requests.get(url, params=params)\n",
    "        yield from gen(json.loads(res.content.decode(\"utf-8\")))\n",
    "\n",
    "##############################################################################\n",
    "# Connectivity sample named tuple,\n",
    "# connectivity sample generator for use with get_msm(),\n",
    "# and dataframe creation function wrapping all of this together\n",
    "##############################################################################\n",
    "\n",
    "TCSample = collections.namedtuple(\"TCSample\",\n",
    "                     (\"msm_id\",\"time\",\"af\",\"proto\",\"pid\",\"sip\",\"dip\",\"reached\",\"hop\",\"rtt\"))\n",
    "\n",
    "def gen_tcs(msm_ary):\n",
    "    for a_res in msm_ary:        \n",
    "        if a_res['type'] == 'traceroute':\n",
    "            if ('result' in a_res):\n",
    "                maxhop = 0\n",
    "                maxhop_ok = False\n",
    "                for hop in a_res['result']:\n",
    "                    if 'result' in hop:\n",
    "                        for pkt in hop['result']:\n",
    "                            if 'from' in pkt:\n",
    "                                maxhop = hop['hop']\n",
    "                                if pkt['from'] == a_res['dst_addr']:\n",
    "                                    maxhop_ok = True\n",
    "                                    if 'rtt' in pkt:\n",
    "                                        rtt = pkt['rtt']\n",
    "                                    else:\n",
    "                                        rtt = -1\n",
    "                                    yield TCSample(a_res['msm_id'],\n",
    "                                           int(a_res['timestamp']), a_res['af'], a_res['proto'],\n",
    "                                           a_res['prb_id'], a_res['src_addr'], a_res['dst_addr'],\n",
    "                                           True, maxhop, rtt)\n",
    "                                    break\n",
    "                if not maxhop_ok:\n",
    "                    try:\n",
    "                        yield TCSample(a_res['msm_id'],\n",
    "                                       int(a_res['timestamp']), a_res['af'], a_res['proto'],\n",
    "                                       a_res['prb_id'], a_res['src_addr'], a_res['dst_addr'],\n",
    "                                       False, maxhop, -1)\n",
    "                    except KeyError:\n",
    "                        # ignore completely broken results\n",
    "                        pass\n",
    "\n",
    "def tcsample_dataframe(msm_ids, cachedir=None, start=None, stop=None, chunksize=1000000):\n",
    "    \"\"\"\n",
    "    Given an iterable of MSMs, create a dataframe of trace connectivity samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize accumulators\n",
    "    adf = pd.DataFrame([], columns=TCSample._fields)\n",
    "    data = []\n",
    "    \n",
    "    # get individual rows from get_msm\n",
    "    for msm_id in msm_ids:\n",
    "        for tcs in get_msm(msm_id, gen=gen_tcs, \n",
    "                           start=start, stop=stop, \n",
    "                           cachedir=cachedir):\n",
    "            data.append(tcs)\n",
    "            \n",
    "            # Append dataframe to dataframe accumulator if chunking.\n",
    "            if len(data) >= chunksize:\n",
    "                adf = adf.append(pd.DataFrame(data, columns=TCSample._fields), ignore_index=True)\n",
    "                data = []\n",
    "                \n",
    "    # Append final dataframe if non-empty.\n",
    "    if len(data) > 0:\n",
    "        adf = adf.append(pd.DataFrame(data, columns=TCSample._fields), ignore_index=True)\n",
    "\n",
    "    # Counter column for aggregation\n",
    "    adf['n'] = 1\n",
    "    \n",
    "    return adf\n",
    "\n",
    "##############################################################################\n",
    "# Plotting utility function\n",
    "##############################################################################\n",
    "\n",
    "def plot_ecdf(a, **kwargs):\n",
    "    sa = np.sort(a)\n",
    "    yv = np.arange(len(sa))/float(len(sa))\n",
    "    plt.plot(sa, yv, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b, 2b: store/restore preparsed raw dataframes in HDF\n",
    "\n",
    "Do this instead of steps 1 and 2 if not reworking parsing and raw dataframs selection. It's silly fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 335 ms, sys: 209 ms, total: 544 ms\n",
      "Wall time: 612 ms\n",
      "CPU times: user 1.25 s, sys: 529 ms, total: 1.78 s\n",
      "Wall time: 2.01 s\n",
      "CPU times: user 160 ms, sys: 87.4 ms, total: 247 ms\n",
      "Wall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "## Restore pre-parsed dataframes from the store, and re-split them as necessary\n",
    "%time tmdf = store['tmdf']\n",
    "%time tcsdf15 = store['tcsdf']\n",
    "%time ut46df = store['feb_tcsdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: find MSM ids for UDP traceroutes \n",
    "\n",
    "Here we limit ourselves to MSMs run (1) from many probes, to maximize the number of probes we'll see samples from, and (2) ending in 2015 or later, to focus on the recent past. Note that many of these are MSMs we specified ourselves for our TMA paper. Not many people seem to be interested in UDP traceroute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get ALL the traceroutes! (takes about five minutes on Forclaz)\n",
    "tmgen = tm_generator()\n",
    "%time tmdf = pd.DataFrame([m for m in tmgen], columns=TraceMeta._fields)\n",
    "tmgen.close() # generator wraps a file, close it.\n",
    "\n",
    "# estimate sample count\n",
    "tmdf['samples'] = (tmdf['probes'] * (tmdf['stop_epoch'] - tmdf['start_epoch'])) / tmdf['interval']\n",
    "\n",
    "# cast timestamps\n",
    "tmdf['start'] = pd.to_datetime(tmdf['start_epoch'] * 1e9)\n",
    "tmdf['stop'] = pd.to_datetime(tmdf['stop_epoch'] * 1e9)\n",
    "tmdf['duration'] = tmdf['stop'] - tmdf['start']\n",
    "\n",
    "# count msms\n",
    "tmdf['n'] = 1\n",
    "\n",
    "# index by msm\n",
    "tmdf.index = pd.Index(tmdf['msm_id'])\n",
    "del(tmdf['msm_id'])\n",
    "\n",
    "# Dump dataframe into the HDF store\n",
    "store['tmdf'] = tmdf\n",
    "\n",
    "# exclude all measurements ending before 1 Jan 2015, and split by protocol\n",
    "tmdf_udp = tmdf[tmdf['proto'] == 'UDP']\n",
    "tmdf_udp = tmdf_udp[tmdf_udp[\"stop\"] >= \"2015-01-01\"]\n",
    "tmdf_tcp = tmdf[tmdf['proto'] == 'TCP']\n",
    "tmdf_tcp = tmdf_tcp[tmdf_tcp[\"stop\"] >= \"2015-01-01\"]\n",
    "tmdf_icmp = tmdf[tmdf['proto'] == 'TCP']\n",
    "tmdf_icmp = tmdf_icmp[tmdf_icmp[\"stop\"] >= \"2015-01-01\"]\n",
    "\n",
    "# find appropriate MSMs\n",
    "msm_ids = tmdf_udp[tmdf_udp['probes'] >= 64].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: Retrieve MSMs and parse them into a dataframe\n",
    "\n",
    "First grab all UDP traceroutes from 2015 on. Then grab UDP and TCP traceroutes from our Feb 2016 run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tcsdf15 = tcsample_dataframe(msm_ids, cachedir=\"data_cache\", chunksize=100000)\n",
    "store['tcsdf'] = tcsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEW_MSMS = range(3354201,3354261)\n",
    "%time ut46df = tcsample_dataframe(NEW_MSMS, cachedir=\"data_cache\", chunksize=100000)\n",
    "store['feb_tcsdf'] = ut46df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: classify 2015 probe IDs by UDP reachability \n",
    "\n",
    "First, clean the data: drop all samples from MSMs where no traceroute reached the target (target always down), then all samples from probes with less than n=9 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tcs(df, min_pid_sample_count=9):\n",
    "    msm_reached_sum = df.groupby('msm_id')['reached'].sum()\n",
    "    bad_target_msm_ids = msm_reached_sum[msm_reached_sum == 0].index\n",
    "    df_target_udp_ok = df[np.logical_not(df['msm_id'].isin(bad_target_msm_ids))]\n",
    "    \n",
    "    pid_sum = df.groupby('pid')['n'].sum()\n",
    "    low_volume_pids = pid_sum[pid_sum < min_pid_sample_count].index\n",
    "    return df_target_udp_ok[np.logical_not(df_target_udp_ok['pid'].isin(low_volume_pids))]\n",
    "\n",
    "tcsdf15_clean = clean_tcs(tcsdf15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load information about the probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = probe_dataframe_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headline number: how many probes might block UDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 / 2240 (3.661%)\n"
     ]
    }
   ],
   "source": [
    "def blocked_pids(df):\n",
    "    reached_by_pid = df.groupby('pid')['reached'].sum()\n",
    "    return reached_by_pid[reached_by_pid == 0].index\n",
    "\n",
    "\n",
    "print (\"%u / %u (%5.3f%%)\" %   (len(blocked_pids(tcsdf15_clean)),\n",
    "                                len(tcsdf15_clean['pid'].unique()),\n",
    "                               100 * (len(blocked_pids(tcsdf15_clean)) / len(tcsdf15_clean['pid'].unique()))))\n",
    "\n",
    "pdf_tcsdf = pdf.loc[tcsdf15_clean['pid'].unique()]\n",
    "pdf_noudp = pdf.loc[blocked_pids(tcsdf15_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   637.,    689.,    720.,   1004.,   1121.,   1270.,   2577.,\n",
       "         2601.,   2844.,   2975.,   3021.,   3174.,   3730.,   4298.,\n",
       "         4554.,   4720.,   4776.,   4833.,   4886.,  10212.,  10977.,\n",
       "        11055.,  12289.,  13061.,  13240.,  13442.,  13712.,  13928.,\n",
       "        14115.,  14390.,  14398.,  14419.,  14454.,  14593.,  14797.,\n",
       "        14889.,  14998.,  15261.,  15342.,  15743.,  15782.,  15942.,\n",
       "        15946.,  15955.,  16053.,  16414.,  16845.,  17009.,  17455.,\n",
       "        17855.,  17938.,  18044.,  18182.,  18292.,  18354.,  18387.,\n",
       "        18550.,  18648.,  19002.,  19201.,  19583.,  19656.,  19774.,\n",
       "        19898.,  19953.,  19998.,  20105.,  20404.,  20423.,  20427.,\n",
       "        20456.,  20479.,  20796.,  20861.,  21170.,  21903.,  22361.,\n",
       "        22471.,  22667.,  22694.,  22948.,  23236.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump the probe IDs into an array\n",
    "pdf_noudp.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all samples from potentially UDP-blocked probes\n",
    "tcsdf_blocked = tcsdf15_clean[tcsdf15_clean['pid'].isin(pdf_noudp.index)]\n",
    "\n",
    "# add sample count, minhop, and maxhop to probe DF for UDP blockage\n",
    "pdf_noudp = pd.merge(pdf_noudp, pd.DataFrame(data = {\"samples\": tcsdf_blocked.groupby('pid')[\"n\"].sum(),\n",
    "                                         \"minhop\":  tcsdf_blocked.groupby('pid')[\"hop\"].min(),\n",
    "                                         \"maxhop\":  tcsdf_blocked.groupby('pid')[\"hop\"].max()}),\n",
    "                     left_index = True, right_index = True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which probes are affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf_noudp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet clear how to classify these based on last hop -- but there's definitely a difference between small hop counts and big ones. Look at AS path distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: check UDP vs TCP with a new measurement study.\n",
    "\n",
    "These were created in the [probe_udp_traceconn_create](probe_udp_traceconn_create.ipynb) notebook. We already loaded them into `ut46df` above. First step is to clean them up, and list probes with various blockages (v6, UDP, TCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aip_map = { \"2001:67c:2e8:11::c100:136b\" : \"ams\",\n",
    "            \"193.0.19.107\" : \"ams\",\n",
    "            \"2401:2000:6660::56\" : \"bne\",\n",
    "            \"203.133.248.56\" : \"bne\" }\n",
    "\n",
    "ut46df['aid'] = ut46df['dip'].apply(lambda x: aip_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_pids(df, pids):\n",
    "    return df[np.logical_not(df['pid'].isin(pids))]\n",
    "\n",
    "# Drop MSMs from probes with too few samples, or failed MSMs\n",
    "ut46df_clean = clean_tcs(ut46df)\n",
    "\n",
    "all_pids = ut46df_clean['pid'].unique()\n",
    "\n",
    "# Find probes that never reached a target, regardless of address family or protocol\n",
    "dead_pids = blocked_pids(ut46df_clean)\n",
    "\n",
    "# Find probes that are alive but never reached a target via IPv6\n",
    "no6_pids = blocked_pids(ut46df_clean[ut46df_clean['af'] == 6]).difference(dead_pids)\n",
    "ignored_pids = dead_pids.union(no6_pids)\n",
    "\n",
    "# Find probes that are alive but never reached a target via IPv4\n",
    "no4_pids = blocked_pids(ut46df_clean[ut46df_clean['af'] == 4]).difference(dead_pids)\n",
    "ignored_pids = ignored_pids.union(no4_pids)\n",
    "\n",
    "# Find probes that are alive but never reached a target via UDP (UDP blocked probes)\n",
    "noudp_pids = blocked_pids(ut46df_clean[ut46df_clean['proto'] == 'UDP']).difference(dead_pids)\n",
    "ignored_pids = ignored_pids.union(noudp_pids)\n",
    "\n",
    "# Find probes that are alive but never reached a target via TCP (TCP blocked probes)\n",
    "notcp_pids = blocked_pids(ut46df_clean[ut46df_clean['proto'] == 'TCP']).difference(dead_pids)\n",
    "ignored_pids = ignored_pids.union(notcp_pids)\n",
    "\n",
    "ut46df_allup = strip_pids(ut46df_clean, ignored_pids)\n",
    "ut46df_reached = ut46df_allup[ut46df_allup[\"reached\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reachability differences only look at probes which can reach all four of v4/v6, UDP/TCP\n",
    "# (maybe should look on a per probe basis too?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npmedian = lambda x: np.percentile(x, 50)\n",
    "\n",
    "# RTT differences only looked at samples where we actually reached the target \n",
    "\n",
    "# first, split on protocol and target\n",
    "ut46df_rs = {}\n",
    "ut46df_mrtt = {}\n",
    "for proto in (\"UDP\", \"TCP\"):\n",
    "    ut46df_rs[proto] = {}\n",
    "    ut46df_mrtt[proto] = {}\n",
    "    for af in (4, 6):\n",
    "        ut46df_rs[proto][af] = {}\n",
    "        ut46df_mrtt[proto][af] = {}\n",
    "        for aid in (\"ams\", \"bne\"):\n",
    "            ut46df_rs[proto][af][aid] = ut46df_reached[(ut46df_reached['proto'] == proto) &\n",
    "                                                       (ut46df_reached['af'] == af) &\n",
    "                                                       (ut46df_reached['aid'] == aid)]\n",
    "            ut46df_mrtt[proto][af][aid] = ut46df_rs[proto][af][aid].groupby('pid')['rtt'].aggregate(npmedian)\n",
    "\n",
    "# now merge for BNE (AMS is broken for TCP...)\n",
    "bne6t = pd.DataFrame(data = ut46df_mrtt['TCP'][6]['bne'])\n",
    "bne6t.columns = ('mrtt_6_tcp',)\n",
    "bne4t = pd.DataFrame(data = ut46df_mrtt['TCP'][4]['bne'])\n",
    "bne4t.columns = ('mrtt_4_tcp',)\n",
    "bne6u = pd.DataFrame(data = ut46df_mrtt['UDP'][6]['bne'])\n",
    "bne6u.columns = ('mrtt_6_udp',)\n",
    "bne4u = pd.DataFrame(data = ut46df_mrtt['UDP'][4]['bne'])\n",
    "bne4u.columns = ('mrtt_4_udp',)\n",
    "\n",
    "mrtt_pid = pd.merge(bne6t, bne4t, left_index=True, right_index=True, how='inner')\n",
    "mrtt_pid = pd.merge(mrtt_pid, bne6u, left_index=True, right_index=True, how='inner')\n",
    "mrtt_pid = pd.merge(mrtt_pid, bne4u, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "len(mrtt_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mrtt_pid[\"6bias_tcp\"] = mrtt_pid[\"mrtt_4_tcp\"] - mrtt_pid[\"mrtt_6_tcp\"]\n",
    "mrtt_pid[\"6bias_udp\"] = mrtt_pid[\"mrtt_4_udp\"] - mrtt_pid[\"mrtt_6_udp\"]\n",
    "mrtt_pid[\"udpbias_4\"] = mrtt_pid[\"mrtt_4_tcp\"] - mrtt_pid[\"mrtt_4_udp\"]\n",
    "mrtt_pid[\"udpbias_6\"] = mrtt_pid[\"mrtt_6_tcp\"] - mrtt_pid[\"mrtt_6_udp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ecdf(mrtt_pid[\"6bias_udp\"])\n",
    "plot_ecdf(mrtt_pid[\"6bias_tcp\"])\n",
    "plt.plot((0,0),(0,1))\n",
    "plt.plot((-60,60),(.5,.5))\n",
    "plt.xlim(-60,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ecdf(mrtt_pid[\"udpbias_4\"], label=\"IPv4\", color=\"red\")\n",
    "plot_ecdf(mrtt_pid[\"udpbias_6\"], label=\"IPv6\", color=\"blue\")\n",
    "plt.plot((0,0),(0,1),color=\"grey\")\n",
    "plt.xlabel(\"UDP RTT bias (ms)\")\n",
    "plt.ylabel(\"ECDF(probes)\")\n",
    "plt.legend(bbox_to_anchor=(0.25, 1))\n",
    "plt.xlim(-10,10)\n",
    "plt.savefig(\"v4v6_udpbias.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map broken probes (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine brokenness by country\n",
    "cc_probe_count = pd.DataFrame(data = {'all': pdf_tcsdf.groupby('cc')['n'].sum(),\n",
    "                                      'no_udp': pdf_noudp.groupby('cc')['n'].sum()} ).fillna(0)   \n",
    "\n",
    "cc_probe_count['badness'] = cc_probe_count[\"no_udp\"] / cc_probe_count[\"all\"]\n",
    "cc_probe_count.sort('badness')\n",
    "cc_probe_count.loc['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map of country codes to shapes\n",
    "import shapefile\n",
    "world_sf = shapefile.Reader(\"data_cache/world/simple\")\n",
    "world_cc = {}\n",
    "for sr in world_sf.shapeRecords():\n",
    "    world_cc[sr.record[1]] = sr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# world map, colored with badness\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "# lon_0 is central longitude of projection.\n",
    "# resolution = 'c' means use crude resolution coastlines.\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.subplots_adjust(left=0.05,right=0.95,top=0.90,bottom=0.05,wspace=0.15,hspace=0.05)\n",
    "ax = plt.subplot(111)\n",
    "m = Basemap(projection='robin',lon_0=0,resolution='c')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='0.9', lake_color='0.95')\n",
    "# draw parallels and meridians.\n",
    "m.drawparallels(np.arange(-90.,120.,30.))\n",
    "m.drawmeridians(np.arange(0.,360.,60.))\n",
    "#x,y = m(pdf_tcsdf['lon'].values,pdf_tcsdf['lat'].values)\n",
    "#ax.scatter(x,y,alpha=0.1,color=\"red\")\n",
    "\n",
    "for cc in cc_probe_count.index.values:\n",
    "    try:\n",
    "        # set color based on data\n",
    "        color = (1.0, 1.0 - cc_probe_count.loc[cc]['badness'], 1.0 - cc_probe_count.loc[cc]['badness'])\n",
    "        \n",
    "        # now parse the shape into a set of projected line segments\n",
    "        shape = world_cc[cc]\n",
    "        polys = [[]]\n",
    "        for n,p in enumerate(shape.points):\n",
    "            # Check to see if we have a new polygon\n",
    "            if n in shape.parts and len(polys[-1]) > 0:\n",
    "                polys.append([])\n",
    "            x,y = m(*p)\n",
    "            polys[-1].append((x,y))\n",
    "        lines = LineCollection(polys, antialiaseds=(1,))\n",
    "        lines.set_facecolors(color)\n",
    "        lines.set_edgecolors((0.2,0,0))\n",
    "        lines.set_linewidth(.5)\n",
    "        ax.add_collection(lines)\n",
    "    except KeyError:\n",
    "        print(\"KeyError for \"+cc)\n",
    "        pass\n",
    "    \n",
    "#m.drawmapboundary(fill_color='0.95')\n",
    "\n",
    "\n",
    "plt.title(\"Distribution of UDP Blocking per Country\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch code\n",
    "\n",
    "Sandbox, dirtpile, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf_noudp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tcsdf_target_udp_ok.groupby('pid')['reached'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ecdf(tcsdf.groupby('pid').reached.sum() / tcsdf.groupby('pid').n.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ecdf(pdf_noudp['minhop'])\n",
    "plot_ecdf(pdf_['maxhop'])\n",
    "plt.xlim(0,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ecdf(tcsdf[tcsdf['reached']]['hop'])\n",
    "plot_ecdf(tcsdf[np.logical_not(tcsdf['reached'])]['hop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendar.timegm((2015,1,1,0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf['n'] = 1\n",
    "pdf_noudp['n'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "as_probe_count = pd.DataFrame(data = {'all': pdf.groupby('asn4')['n'].sum(),\n",
    "                                      'no_udp': pdf_noudp.groupby('asn4')['n'].sum()} ).dropna()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_probe_count[\"udp_brokenness\"] = cc_probe_count['no_udp'] / cc_probe_count['all']\n",
    "as_probe_count[\"udp_brokenness\"] = as_probe_count['no_udp'] / as_probe_count['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "as_probe_count.sort(\"udp_brokenness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_probe_count.sort(\"udp_brokenness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcsdf.groupby('probe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
